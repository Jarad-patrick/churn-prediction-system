{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  \n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],\n",
    "    'reg_alpha': [0, 0.01, 0.1],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,             # number of random combinations\n",
    "    scoring='roc_auc',     # maximize AUC\n",
    "    cv=3,                  # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(x_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Best AUC:\", random_search.best_score_)\n",
    "\n",
    "best_xgb = random_search.best_estimator_\n",
    "y_pred_best = best_xgb.predict(x_test_scaled)\n",
    "y_proba_best = best_xgb.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "print(\"Test AUC:\", roc_auc_score(y_test, y_proba_best))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_best))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  \n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "\n",
    "#XGBOOST MODEL\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "\n",
    "xgb.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "xgb_pred = xgb.predict(x_test_scaled)\n",
    "xgb_proba = xgb.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "xgb_auc = roc_auc_score(y_test, xgb_proba)\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "xgb_cm = confusion_matrix(y_test, xgb_pred)\n",
    "\n",
    "print(\"AUC:\", xgb_auc)\n",
    "print(\"Accuracy:\", xgb_acc)\n",
    "print(\"Confusion Matrix:\\n\", xgb_cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, xgb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of RF\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  \n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,       \n",
    "    max_depth= None,      \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model.fit(x_train_scaled,y_train)\n",
    "y_pred_rf = rf_model.predict(x_test_scaled)\n",
    "y_pred_proba_rf = rf_model.predict_proba(x_test_scaled)[:, 1] \n",
    "\n",
    "#Feature importance\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': x.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(feature_importances)\n",
    "\n",
    "# Plot top 10 features\n",
    "feature_importances.head(10).plot(kind='barh', x='Feature', y='Importance', legend=False, color='skyblue')\n",
    "plt.title(\"Top 10 Important Features for Churn\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation of RF\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  \n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,       \n",
    "    max_depth= None,      \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_model.fit(x_train_scaled,y_train)\n",
    "y_pred_rf = rf_model.predict(x_test_scaled)\n",
    "y_pred_proba_rf = rf_model.predict_proba(x_test_scaled)[:, 1] \n",
    "\n",
    "\n",
    "# Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# ROC Curve & AUC\n",
    "auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_rf)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.title(f\"Random Forest ROC Curve (AUC = {auc_rf:.2f})\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  \n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier   #using random forest classification to test the model again\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,       # number of trees   100 decision trees in the forest\n",
    "    max_depth= None,        # trees can grow until leaves are pure \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#train the model\n",
    "rf_model.fit(x_train_scaled,y_train)\n",
    "\n",
    "#make predictions\n",
    "y_pred_rf = rf_model.predict(x_test_scaled)\n",
    "y_pred_proba_rf = rf_model.predict_proba(x_test_scaled)[:, 1]  # churn probability\n",
    "\n",
    "print(y_pred_proba_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  \n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit (x_train_scaled,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test_scaled) \n",
    "y_pred_prob = model.predict_proba(x_test_scaled)[:,1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve  #the ROC  Curve synonymous to the t-distribution\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0,1], [0,1], linestyle='--')\n",
    "plt.title(f\"ROC Curve (AUC = {auc:.2f})\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  #\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit (x_train_scaled,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test_scaled) \n",
    "y_pred_prob = model.predict_proba(x_test_scaled)[:,1]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))   #classification report evaluates how well your model did on the test set\n",
    "#precision of all the customers the model predicted would churn, how many actually churned?\n",
    "#recall(sensitivity) of all the customers who actually churned how many did the model correctly predict?  *most important metric\n",
    "#F1 score a balance between precision and recall. Useful if you care about both false alarms and misses.\n",
    "#support the actual number of samples in y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  #\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit (x_train_scaled,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test_scaled) \n",
    "y_pred_prob = model.predict_proba(x_test_scaled)[:,1]\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred) #gives the confusion matrix for the predictions\n",
    "cm   # TN FP FN TP   we want to reduce FN as much as possible because it is a dangerous decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  #\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit (x_train_scaled,y_train)\n",
    "\n",
    "\n",
    "#Make predictions\n",
    "y_pred = model.predict(x_test_scaled) \n",
    "y_pred_prob = model.predict_proba(x_test_scaled)[:,1] #Gives the Probability that each customer will churn 0-1\n",
    "print(y_pred_prob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  #\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit (x_train_scaled,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test_scaled) \n",
    "\n",
    "#Interpretation of the model using logistic regression \n",
    "import numpy as np \n",
    "coeffs = pd.DataFrame({\n",
    "    'Feature':x.columns,      #-> list of all input feature columns         \n",
    "    'Coefficient': model.coef_[0]  #-> the weights/importance learned by the logistic regression model each feature gets a co efficient \n",
    "})\n",
    "\n",
    "coeffs.sort_values(by='Coefficient', ascending=False)    #Features close to 0 have very little impact\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  #\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit (x_train_scaled,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test_scaled) \n",
    "\n",
    "#Evaluation of the model to determine how good the model is so we use: Accuracy_score, Confusion_matrix, Classification_report \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print('accuracy:', accuracy_score(y_pred,y_test))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  \n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit (x_train_scaled,y_train)\n",
    "\n",
    "y_pred = model.predict(x_test_scaled) #to make predictions for the test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  \n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "#brings in the ML library used for modelling \n",
    "from sklearn.linear_model import LogisticRegression   #the logistic regression fucntion is used to estimate the probability that an observation belongs to a particular class.\n",
    "model = LogisticRegression(max_iter=1000) #trys the code up to 1000 times to find the best possible solution\n",
    "model.fit (x_train_scaled,y_train) # .fit() teaches the model patterns from the data  x_train-> the input features, y_train-> the output target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)  #Numeric features like MonthlyPremium or PolicyTenure may vary in range. Scaling helps models converge faster.\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "x = df_encoded.drop('churn', axis=1)\n",
    "y = df_encoded['churn'] \n",
    "\n",
    "\n",
    "#the train_test_split function splits the dataset into a training and testing sets\n",
    "#test_size=0.2 means 20% of the data will be used for testing and 80% will be used for training \n",
    "#randomstate = 42- it makes the datset reproducible meaning if you run it multiple times you will get the same split.\n",
    "# x_test = features for testing 20% of the data, x_train = features for training 80% of the data\n",
    "# y_test = target values for testing, y_train = target values for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state= 42 \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True)\n",
    "\n",
    "#Split into Features and Target\n",
    "x = df_encoded.drop('churn', axis=1) #it takes in all other columns and removes the churn column from the data set -think of it as everything that will help predict the churn apart from the actual churn \n",
    "y = df_encoded['churn'] #the target variable- the actual churn what i want the model to learn to predict using (x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "categorical_cols = ['gender','region','marital_status','policy_type']#Machine learning models only understand numbers. So we need to convert categories into numeric form.\n",
    "df_encoded = pd.get_dummies(df,categorical_cols,drop_first=True) #converting from text to numerical figures now all categories become 0s and 1s  p.s this is a one-hot encoding\n",
    "#it also creates a new virtual dataframe where all figures are numeric \n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "# Satisfaction vs Churn\n",
    "sns.barplot(x='satisfaction_score', y='churn', data=df)\n",
    "plt.show()\n",
    "\n",
    "# Complaints vs Churn\n",
    "sns.barplot(x='complaints_filed', y='churn', data=df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "df.groupby('policy_type')['churn'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "df.groupby('policy_tenure')['churn'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "df.groupby('complaints_filed')['churn'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "df.groupby('satisfaction_score')['churn'].mean() #groups the customer satisfaction by churn output and shows the percentage of customer satisfaction by churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "\n",
    "\n",
    "df['churn'].value_counts().plot(kind='bar', color=['skyblue', 'orange'])#looks at the churn column counts how many customers there are i.e 1,0 and also creates the bar chart\n",
    "plt.title('Churn Distribution')\n",
    "plt.xlabel('churn')\n",
    "plt.ylabel('Number of customers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "\n",
    "df['churn'] = df['churn'].astype(int)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "\n",
    "\n",
    "\n",
    "df['churn'].value_counts() #counts how many customers churned \n",
    "df['churn'].value_counts(normalize=True) * 100 # normalize=true converts it to percentage because pandas sees it as a proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "\n",
    "df.isnull().sum() #used to check if any columns have empty values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "df.shape # returns the total number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('insurance_churn_data (1).csv')\n",
    "df.head()   # returns the first 5 columns and rows\n",
    "df.describe() #Gives a statistical summary of the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import numpy as np "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
